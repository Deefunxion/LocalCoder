# Academicon Code Assistant - Configuration Template
# Copy this file to .env and fill in your values

# ============================================================================
# REQUIRED: OpenRouter API Configuration
# ============================================================================
# Get your API key from: https://openrouter.ai/
# Free tier available with generous token limits
OPENROUTER_API_KEY=sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# ============================================================================
# LLM MODEL SELECTION (OpenRouter Models)
# ============================================================================
# Orchestrator: Plans search strategy (fast, low cost)
# Available free models: google/gemini-2.0-flash-exp-1219, deepseek/deepseek-chat-v3.1:free
# Default: google/gemini-2.0-flash-exp-1219 (free, fast, reliable)
ORCHESTRATOR_MODEL=google/gemini-2.0-flash-exp-1219

# Graph Analyst: Analyzes code relationships (optional, currently disabled)
# Default: openai/gpt-3.5-turbo (fast, cheap, paid)
GRAPH_ANALYST_MODEL=openai/gpt-3.5-turbo

# Synthesizer: Generates final answer (most important)
# Default: openai/gpt-4-turbo-preview (best quality, paid)
SYNTHESIZER_MODEL=openai/gpt-4-turbo-preview

# Available free models on OpenRouter:
# - google/gemini-2.0-flash-exp-1219  (Very fast, good quality)
# - deepseek/deepseek-chat-v3.1:free  (Good coding knowledge)
# - z-ai/glm-4.5-air:free             (Fast)
# 
# Paid models (recommended):
# - openai/gpt-4-turbo-preview        (Best quality)
# - google/gemini-2.0-pro-exp-1219    (Fast and smart)

# ============================================================================
# MODEL BEHAVIOR TUNING
# ============================================================================
# Orchestrator temperature: Lower = more deterministic planning (0.0-1.0)
ORCHESTRATOR_TEMPERATURE=0.1

# Graph Analyst temperature: Higher = more creative analysis
GRAPH_ANALYST_TEMPERATURE=0.5

# Synthesizer temperature: Low = focused answers, High = creative
SYNTHESIZER_TEMPERATURE=0.2

# ============================================================================
# REQUEST TIMEOUTS (seconds)
# ============================================================================
# Time to wait for Orchestrator response
ORCHESTRATOR_TIMEOUT=60

# Time to wait for Graph Analyst response
GRAPH_ANALYST_TIMEOUT=120

# Time to wait for Synthesizer response (longest, ~5-8 seconds typical)
SYNTHESIZER_TIMEOUT=90

# ============================================================================
# CACHE DIRECTORIES (Optional - auto-detected if not set)
# ============================================================================
# Main cache base directory (Windows: D:/AI-Models, Linux: ~/.cache/academicon)
# HF_HOME=D:/AI-Models/huggingface-moved
# TRANSFORMERS_CACHE=D:/AI-Models/transformers
# SENTENCE_TRANSFORMERS_HOME=D:/AI-Models/embeddings

# ============================================================================
# DATABASE CONFIGURATION (Optional - auto-detected if not set)
# ============================================================================
# Vector database path (default: ./academicon_chroma_db)
# DB_PATH=./academicon_chroma_db

# Vector database collection name (default: academicon_code)
# DB_COLLECTION_NAME=academicon_code

# ============================================================================
# CODEBASE INDEXING (Optional - for index_academicon_lite.py)
# ============================================================================
# Path to Academicon codebase (WSL path for Windows users)
# ACADEMICON_PATH=//wsl$/Ubuntu/home/deeznutz/projects/Academicon-Rebuild

# File extensions to index (comma-separated, default: .py only for speed)
# INDEX_FILE_EXTENSIONS=.py

# ============================================================================
# RAG PIPELINE SETTINGS (Optional - rarely need adjustment)
# ============================================================================
# Number of code chunks to retrieve per query
RETRIEVAL_TOP_K=5

# Size of conversation history to maintain (for context)
CONVERSATION_HISTORY_SIZE=3

# Chunk size for code splitting (tokens)
CHUNK_SIZE=2048

# Overlap between chunks (for context preservation)
CHUNK_OVERLAP=256

# ============================================================================
# NOTES
# ============================================================================
# 1. REQUIRED: Only OPENROUTER_API_KEY must be set
# 2. All other variables have sensible defaults in config/settings.py
# 3. GPU acceleration is auto-detected (CUDA if available, CPU fallback)
# 4. Model costs vary - check https://openrouter.ai/prices before using
# 5. Conversation history is temporary (cleared each session)
