# Î£ÏÎ½Î¿ÏˆÎ· Î”Î¹ÏŒÏÎ¸Ï‰ÏƒÎ·Ï‚ - Academicon Indexing

**Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±:** 12 ÎÎ¿ÎµÎ¼Î²ÏÎ¯Î¿Ï… 2025
**Î ÏÏŒÎ²Î»Î·Î¼Î±:** CUDA Out of Memory ÎºÎ±Ï„Î¬ Ï„Î¿ indexing Ï„Î¿Ï… Academicon codebase

---

## ğŸ¯ Î¤Î¹ Î”Î¹Î¿ÏÎ¸ÏÎ¸Î·ÎºÎµ

### ÎšÏÏÎ¹Î¿ Î ÏÏŒÎ²Î»Î·Î¼Î±
Î¤Î¿ GPU batch size Ï…Ï€Î¿Î»Î¿Î³Î¹Î¶ÏŒÏ„Î±Î½ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± ÏƒÎµ **307**, Ï€Î¿Ï… Î®Ï„Î±Î½ Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î¿ Î³Î¹Î± Ï„Î¿ Academicon codebase (25,870 chunks). ÎŒÏ„Î±Î½ Ï„Î¿ Ollama LLM Î®Ï„Î±Î½ loaded (~8-12GB VRAM), Î´ÎµÎ½ Î­Î¼ÎµÎ½Îµ Î±ÏÎºÎµÏ„ÏŒ VRAM Î³Î¹Î± Ï„Î± embeddings.

### Î›ÏÏƒÎ·
Î ÏÎ¿ÏƒÏ„Î­Î¸Î·ÎºÎµ **conservative mode** ÏƒÏ„Î¿Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ Ï„Î¿Ï… batch size Ï€Î¿Ï…:
- Î ÎµÏÎ¹Î¿ÏÎ¯Î¶ÎµÎ¹ Ï„Î¿ batch size ÏƒÏ„Î¿ **64** Î³Î¹Î± Î¼ÎµÎ³Î¬Î»Î± codebases
- Î”ÎµÏƒÎ¼ÎµÏÎµÎ¹ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿ VRAM Î³Î¹Î± Ï„Î¿ Ollama (8GB Î±Î½Ï„Î¯ Î³Î¹Î± 6GB)
- Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î± ÏƒÏ„Î±Î¼Î±Ï„Î¬ÎµÎ¹ Ï„Î¿ Ollama Ï€ÏÎ¹Î½ Ï„Î¿ indexing

---

## ğŸ“ Î‘Î»Î»Î±Î³Î­Ï‚ ÏƒÏ„Î¿Î½ ÎšÏÎ´Î¹ÎºÎ±

### 1. `src/utils/gpu_utils.py`
```python
# Î Î¡ÎŸÎ£Î˜Î—ÎšÎ— Î½Î­Î±Ï‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…
def calculate_optimal_batch_size(
    model_size_gb: float = 2.0,
    reserve_vram_gb: Optional[float] = None,
    conservative: bool = False  # ÎÎ•ÎŸ!
) -> int:
    # ...
    if conservative:
        batch_size = min(batch_size, 64)  # Cap ÏƒÏ„Î¿ 64
```

### 2. `index_academicon_v2.py`
```python
# Î Î¡Î™Î
device, batch_size = get_device_and_batch_size()  # â†’ 307 âŒ

# Î¤Î©Î¡Î‘
batch_size = calculate_optimal_batch_size(
    model_size_gb=2.0,
    reserve_vram_gb=8.0,      # Î ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿ reserve
    conservative=True          # Î‘ÏƒÏ†Î±Î»Î­Ï‚ batch size
)  # â†’ 64 âœ…
```

### 3. `index_academicon_lite.py`
```python
# Î Î¡Î™Î
batch_size = 128 if device == "cuda" else 32  # 128 Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î¿ âŒ

# Î¤Î©Î¡Î‘
batch_size = 64  # Conservative Î³Î¹Î± Academicon âœ…
```

---

## ğŸ†• ÎÎ­Î± Scripts

### 1. `check_vram.py`
Î•Î»Î­Î³Ï‡ÎµÎ¹ VRAM usage ÎºÎ±Î¹ Î´Î¯Î½ÎµÎ¹ ÏƒÏ…ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚:
```bash
python check_vram.py
```

**Output:**
```
GPU: NVIDIA GeForce RTX 5070 Ti
Total VRAM:     16.0 GB
Free:           14.2 GB

âœ… Ollama is NOT running - good for indexing!

ğŸ“Š BATCH SIZE RECOMMENDATIONS:
   LARGE codebase (>15,000 chunks) - ACADEMICON:
     Recommended batch_size: 64
```

### 2. `index_academicon.bat`
Î‘Ï…Ï„Î¿Î¼Î±Ï„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ script Ï€Î¿Ï…:
- âœ… Î£Ï„Î±Î¼Î±Ï„Î¬ÎµÎ¹ Ï„Î¿ Ollama Ï€ÏÏÏ„Î± (Î½Î± ÎµÎ»ÎµÏ…Î¸ÎµÏÏÏƒÎµÎ¹ VRAM)
- âœ… Î¤ÏÎ­Ï‡ÎµÎ¹ `index_academicon_v2.py` Î¼Îµ batch_size=64
- âœ… Î”Î¯Î½ÎµÎ¹ Î¿Î´Î·Î³Î¯ÎµÏ‚ Î³Î¹Î± restart Ollama Î¼ÎµÏ„Î¬

```bash
index_academicon.bat
```

### 3. `index_academicon_lite.bat`
Î“Î¹Î± Î³ÏÎ·Î³Î¿ÏÏŒÏ„ÎµÏÎ¿ indexing (Î¼ÏŒÎ½Î¿ Python files):
```bash
index_academicon_lite.bat
```

---

## ğŸš€ Î ÏÏ‚ Î½Î± Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚

### Î’Î®Î¼Î± 1: ÎˆÎ»ÎµÎ³Î¾Îµ VRAM
```bash
python check_vram.py
```

### Î’Î®Î¼Î± 2: ÎšÎ¬Î½Îµ Index (ÎµÏ€Î¯Î»ÎµÎ¾Îµ Î­Î½Î±)

**Option A - Full Indexing** (ÏŒÎ»Î± Ï„Î± Î±ÏÏ‡ÎµÎ¯Î±):
```bash
index_academicon.bat
```
Î§ÏÏŒÎ½Î¿Ï‚: ~15-20 Î»ÎµÏ€Ï„Î¬

**Option B - Lite Indexing** (Î¼ÏŒÎ½Î¿ Python):
```bash
index_academicon_lite.bat
```
Î§ÏÏŒÎ½Î¿Ï‚: ~10-15 Î»ÎµÏ€Ï„Î¬

### Î’Î®Î¼Î± 3: Test Ï„Î¿ System
```bash
# Î‘Ï†Î¿Ï Ï„ÎµÎ»ÎµÎ¹ÏÏƒÎµÎ¹ Ï„Î¿ indexing, restart Ollama
ollama serve

# Î¤ÏÎ­Î¾Îµ Ï„Î¿ assistant
python main.py
```

---

## ğŸ“Š Performance

### Batch Size Comparison

| Scenario | Batch Size Î ÏÎ¹Î½ | Batch Size Î¤ÏÏÎ± | VRAM Usage | Result |
|----------|-----------------|-----------------|------------|--------|
| Local test (small) | 128 | 128 | ~4GB | âœ… OK |
| Academicon (large) | 307 | 64 | ~6GB | âœ… FIXED! |
| With Ollama running | 307 | 32-64 | ~14GB | âœ… SAFE |

### Expected Indexing Times

**ÎœÎµ GPU (RTX 5070 Ti):**
- Lite version (Python): 10-15 Î»ÎµÏ€Ï„Î¬
- Full version (ÏŒÎ»Î±): 15-20 Î»ÎµÏ€Ï„Î¬
- Chunks/second: ~25-30

**ÎœÎµ CPU (fallback):**
- Lite version: 30-45 Î»ÎµÏ€Ï„Î¬
- Full version: 60-90 Î»ÎµÏ€Ï„Î¬
- Chunks/second: ~5-8

---

## ğŸ’¡ Best Practices

### Î ÏÎ¹Î½ Ï„Î¿ Indexing:
1. âœ… Check VRAM: `python check_vram.py`
2. âœ… Close Ollama: `taskkill /IM ollama.exe /F`
3. âœ… Close Î¬Î»Î»Î± GPU apps

### Î‘Î½ Î Î¬Î»Î¹ Î Î­ÏƒÎµÎ¹ OOM:
1. ÎœÎµÎ¯Ï‰ÏƒÎµ batch_size ÏƒÎµ 32 ÏƒÏ„Î¿ `index_academicon_v2.py`
2. Î‰ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ CPU: `device="cpu"`
3. Î‰ restart Ï„Î¿Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î® (clears GPU memory)

---

## ğŸ“ Î‘ÏÏ‡ÎµÎ¯Î± Ï€Î¿Ï… Î‘Î»Î»Î¬Ï‡Ï„Î·ÎºÎ±Î½

### Î¤ÏÎ¿Ï€Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î±:
- âœ… `src/utils/gpu_utils.py` - Conservative mode
- âœ… `index_academicon_v2.py` - Batch size 64
- âœ… `index_academicon_lite.py` - Batch size 64
- âœ… `IMPLEMENTATION_PROGRESS.md` - Bug fix log

### Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎ±Î½:
- âœ… `check_vram.py` - VRAM checker
- âœ… `index_academicon.bat` - Auto full indexing
- âœ… `index_academicon_lite.bat` - Auto lite indexing
- âœ… `ACADEMICON_INDEXING_FIX.md` - Detailed docs (Greek)
- âœ… `GREEK_INDEXING_SUMMARY.md` - Î‘Ï…Ï„ÏŒ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿

---

## âœ… Verification

Î“Î¹Î± Î½Î± ÎµÏ€Î¹Î²ÎµÎ²Î±Î¹ÏÏƒÎµÎ¹Ï‚ ÏŒÏ„Î¹ Î´Î¿Ï…Î»ÎµÏÎµÎ¹:

```bash
# 1. Check GPU
python check_vram.py

# 2. Run indexing
index_academicon_lite.bat

# 3. Verify database
ls academicon_chroma_db

# 4. Test queries
python main.py
```

---

## ğŸ‰ Î‘Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±

Î¤Î¿ Academicon codebase Ï„ÏÏÎ± ÎºÎ¬Î½ÎµÎ¹ index **Ï‡Ï‰ÏÎ¯Ï‚ CUDA OOM errors**!

**Next Step:** Î¤ÏÎ­Î¾Îµ `index_academicon_lite.bat` ÎºÎ±Î¹ ÏƒÎµ 10-15 Î»ÎµÏ€Ï„Î¬ Î¸Î± ÎµÎ¯ÏƒÎ±Î¹ Î­Ï„Î¿Î¹Î¼Î¿Ï‚ Î½Î± ÎºÎ¬Î½ÎµÎ¹Ï‚ queries! ğŸš€

---

**Î•ÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚;** Î”ÎµÏ‚ Ï„Î¿ `ACADEMICON_INDEXING_FIX.md` Î³Î¹Î± Î»ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚.
