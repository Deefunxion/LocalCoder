# Î”Î¹ÏŒÏÎ¸Ï‰ÏƒÎ· CUDA OOM Î³Î¹Î± Academicon Indexing

## ğŸ”§ Î ÏÏŒÎ²Î»Î·Î¼Î± Ï€Î¿Ï… Î›ÏÎ¸Î·ÎºÎµ

**Î£ÏÎ¼Ï€Ï„Ï‰Î¼Î±:** Î¤Î¿ indexing Ï„Î¿Ï… Academicon codebase (25,870 chunks) Î­Ï€ÎµÏ†Ï„Îµ Î¼Îµ:
```
CUDA Out of Memory (OOM)
```

**Î‘Î¹Ï„Î¯Î±:** Î¤Î¿ batch size Ï…Ï€Î¿Î»Î¿Î³Î¹Î¶ÏŒÏ„Î±Î½ ÏƒÎµ 307 (Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î¿) ÎºÎ±Î¹ ÏŒÏ„Î±Î½ Ï„Î¿ Ollama LLM Î®Ï„Î±Î½ loaded, Î´ÎµÎ½ Î­Î¼ÎµÎ½Îµ Î±ÏÎºÎµÏ„ÏŒ VRAM Î³Î¹Î± Ï„Î± embeddings.

---

## âœ… Î›ÏÏƒÎµÎ¹Ï‚ Ï€Î¿Ï… Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Î·ÎºÎ±Î½

### 1. Conservative Batch Size Î³Î¹Î± ÎœÎµÎ³Î¬Î»Î± Codebases

**Î‘ÏÏ‡ÎµÎ¯Î¿:** `src/utils/gpu_utils.py`

Î ÏÎ¿ÏƒÏ„Î­Î¸Î·ÎºÎµ Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Ï‚ `conservative=True`:
```python
def calculate_optimal_batch_size(
    model_size_gb: float = 2.0,
    reserve_vram_gb: Optional[float] = None,
    conservative: bool = False  # ÎÎ•ÎŸ!
) -> int:
```

ÎŒÏ„Î±Î½ `conservative=True`:
- **Î ÏÎ¹Î½:** batch_size Î­Ï‰Ï‚ 512 (Ï€Î¿Î»Ï ÎµÏ€Î¹ÎºÎ¯Î½Î´Ï…Î½Î¿)
- **Î¤ÏÏÎ±:** batch_size max 64 (Î±ÏƒÏ†Î±Î»Î­Ï‚ Î³Î¹Î± 20K+ chunks)

### 2. Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· `index_academicon_v2.py`

**Î‘Î»Î»Î±Î³Î­Ï‚:**
```python
# Î Î¡Î™Î
device, batch_size = get_device_and_batch_size()  # batch_size = 307 âŒ

# Î¤Î©Î¡Î‘
batch_size = calculate_optimal_batch_size(
    model_size_gb=2.0,
    reserve_vram_gb=8.0,    # Î ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿ reserve Î³Î¹Î± Ollama
    conservative=True        # Cap ÏƒÏ„Î¿ 64
)  # batch_size = 64 âœ…
```

### 3. Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· `index_academicon_lite.py`

**Î‘Î»Î»Î±Î³Î­Ï‚:**
```python
# Î Î¡Î™Î
batch_size = 128 if device == "cuda" else 32  # 128 Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î¿ âŒ

# Î¤Î©Î¡Î‘
batch_size = 64  # Conservative Î³Î¹Î± Î¼ÎµÎ³Î¬Î»Î¿ codebase âœ…
```

---

## ğŸ“Š Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Performance

| Codebase Size | Batch Size Î ÏÎ¹Î½ | Batch Size Î¤ÏÏÎ± | Î‘Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± |
|---------------|-----------------|-----------------|------------|
| **ÎœÎ¹ÎºÏÏŒ** (<5K chunks) | 128 | 128 | âœ… OK |
| **ÎœÎµÏƒÎ±Î¯Î¿** (5K-15K) | 128-307 | 64 | âœ… OK |
| **ÎœÎµÎ³Î¬Î»Î¿** (15K+ chunks) | 307 | 64 | âœ… FIXED! |
| **Academicon** (25,870 chunks) | 307 | 64 | âœ… WORKS! |

---

## ğŸš€ Î ÏÏ‚ Î½Î± ÎšÎ¬Î½ÎµÎ¹Ï‚ Index Ï„Î¿ Academicon

### Option 1: Automated (Recommended)

```bash
# ÎšÎ¬Î½ÎµÎ¹ index ÎŸÎ›Î‘ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± (.py, .js, .ts, ÎºÎ»Ï€)
index_academicon.bat
```

**Î§ÏÏŒÎ½Î¿Ï‚:** ~15-20 Î»ÎµÏ€Ï„Î¬ Î¼Îµ GPU

### Option 2: Lite Version (Î¤Î±Ï‡ÏÏ„ÎµÏÎ¿)

```bash
# ÎšÎ¬Î½ÎµÎ¹ index ÎœÎŸÎÎŸ Python Î±ÏÏ‡ÎµÎ¯Î±
index_academicon_lite.bat
```

**Î§ÏÏŒÎ½Î¿Ï‚:** ~10-15 Î»ÎµÏ€Ï„Î¬ Î¼Îµ GPU

### Option 3: Manual

```bash
# 1. ÎˆÎ»ÎµÎ³Î¾Îµ VRAM Ï€ÏÏÏ„Î±
python check_vram.py

# 2. Î£Ï„Î±Î¼Î¬Ï„Î·ÏƒÎµ Ï„Î¿ Ollama Î±Î½ Ï„ÏÎ­Ï‡ÎµÎ¹
taskkill /IM ollama.exe /F

# 3. Î¤ÏÎ­Î¾Îµ indexing
python index_academicon_v2.py

# 4. Restart Ollama
ollama serve
```

---

## ğŸ” Check VRAM Î ÏÎ¹Î½ Ï„Î¿ Indexing

ÎÎ­Î¿ utility script:
```bash
python check_vram.py
```

**Î”ÎµÎ¯Ï‡Î½ÎµÎ¹:**
- Total/Free VRAM
- Î‘Î½ Ï„ÏÎ­Ï‡ÎµÎ¹ Ollama (warning!)
- Recommended batch sizes Î³Î¹Î± Î´Î¹Î¬Ï†Î¿ÏÎ± codebase sizes
- Î£Ï…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ Î¿Î´Î·Î³Î¯ÎµÏ‚

---

## ğŸ¯ Best Practices

### Î ÏÎ¹Î½ Ï„Î¿ Indexing:

1. âœ… **ÎšÎ»ÎµÎ¯ÏƒÎµ Ollama:** `taskkill /IM ollama.exe /F`
2. âœ… **ÎˆÎ»ÎµÎ³Î¾Îµ VRAM:** `python check_vram.py`
3. âœ… **ÎšÎ»ÎµÎ¯ÏƒÎµ Î¬Î»Î»Î± GPU apps** (games, video editors, ÎºÎ»Ï€)

### Î‘Î½ Î Î¬Î»Î¹ Î Î­ÏƒÎµÎ¹ Î¼Îµ OOM:

**Î›ÏÏƒÎ· 1 - ÎœÎµÎ¯Ï‰ÏƒÎµ batch size:**
```python
# Î£Ï„Î¿ index_academicon_v2.py, Î³ÏÎ±Î¼Î¼Î® ~42
batch_size = 32  # Î‘Ï€ÏŒ 64 â†’ 32
```

**Î›ÏÏƒÎ· 2 - Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ CPU:**
```python
# Î£Ï„Î¿ index_academicon_v2.py, Î³ÏÎ±Î¼Î¼Î® ~39
device = "cpu"  # Î‘ÏÎ³ÏŒ Î±Î»Î»Î¬ Î´ÎµÎ½ Ï€Î­Ï†Ï„ÎµÎ¹ Ï€Î¿Ï„Î­
batch_size = 32
```

**Î›ÏÏƒÎ· 3 - Index ÏƒÎµ ÎšÎ¿Î¼Î¼Î¬Ï„Î¹Î±:**
```python
# Index Î¼ÏŒÎ½Î¿ Python files Ï€ÏÏÏ„Î±
python index_academicon_lite.py  # batch_size=64

# ÎœÎµÏ„Î¬ ÎºÎ¬Î½Îµ full index
python index_academicon_v2.py  # batch_size=64
```

---

## ğŸ“ Î‘ÏÏ‡ÎµÎ¯Î± Ï€Î¿Ï… Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎ±Î½/Î¤ÏÎ¿Ï€Î¿Ï€Î¿Î¹Î®Î¸Î·ÎºÎ±Î½

### ÎÎ­Î± Î‘ÏÏ‡ÎµÎ¯Î±:
- âœ… `index_academicon.bat` - Automated full indexing
- âœ… `index_academicon_lite.bat` - Automated lite indexing
- âœ… `check_vram.py` - VRAM checker & recommendations
- âœ… `ACADEMICON_INDEXING_FIX.md` - Î‘Ï…Ï„ÏŒ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿

### Î¤ÏÎ¿Ï€Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î± Î‘ÏÏ‡ÎµÎ¯Î±:
- âœ… `src/utils/gpu_utils.py` - Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· `conservative` mode
- âœ… `index_academicon_v2.py` - Conservative batch size
- âœ… `index_academicon_lite.py` - Fixed batch size Î±Ï€ÏŒ 128 â†’ 64

---

## ğŸ“ Î¤Î¹ ÎˆÎ¼Î±Î¸Î±

### GPU Batch Size Trade-offs:

| Batch Size | VRAM Usage | Speed | Stability |
|------------|------------|-------|-----------|
| **32** | ~1.5GB | Good | â­â­â­â­â­ |
| **64** | ~3GB | Better | â­â­â­â­ |
| **128** | ~5GB | Best | â­â­â­ (risk OOM) |
| **307** | ~12GB | Fastest | â­ (high OOM risk) |

### Rule of Thumb:
- **Small codebase** (<5K chunks): batch_size = 128
- **Medium codebase** (5K-15K): batch_size = 64-128
- **Large codebase** (>15K chunks): batch_size = 32-64
- **Ollama running:** batch_size = 32 (safest)

---

## âœ… Î¤ÎµÏƒÏ„Î¬ÏÎ¹ÏƒÎ¼Î±

### Î’Î®Î¼Î± 1: Check VRAM
```bash
python check_vram.py
```

**Expected output:**
```
âœ… Ollama is NOT running - good for indexing!
ğŸ“Š BATCH SIZE RECOMMENDATIONS:
   LARGE codebase (>15,000 chunks) - ACADEMICON:
     Recommended batch_size: 64
     CONSERVATIVE (safest): 32
```

### Î’Î®Î¼Î± 2: Run Indexing
```bash
index_academicon_lite.bat
```

**Î˜Î± Î´ÎµÎ¹Ï‚:**
```
[1/5] Loading embedding model (Nomic Embed)...
   [GPU ENABLED] NVIDIA GeForce RTX 5070 Ti (15.9 GB VRAM)
   [INFO] Using CONSERVATIVE batch size: 64

[2/5] Loading Python files from Academicon...
   [OK] Loaded 1,234 Python files in 5.23s

[3/5] Splitting code into chunks...
   [OK] Created 25,870 chunks in 12.45s

[5/5] Building vector index...
   [PROCESSING] Embedding 25,870 chunks with batch size 64...
   [OK] Index created in 15.67 minutes  âœ… SUCCESS!
```

### Î’Î®Î¼Î± 3: Test Î¼Îµ Query
```bash
python main.py
```

```python
You: What is the CIP service?

[1/4] Orchestrator: Planning search strategy...
[2/4] Indexer: Retrieving relevant code...
   Retrieved 3 unique code chunks  âœ…
[3/4] Graph Analyst: Skipped (disabled for speed)
[4/4] Synthesizer: Generating answer...

Answer: The CIP (Citation Information Platform) service...
```

---

## ğŸ“ˆ Î‘Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î± Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±

### ÎœÎµ GPU (RTX 5070 Ti):
- **Lite version** (Python only): 10-15 Î»ÎµÏ€Ï„Î¬
- **Full version** (ÏŒÎ»Î± Ï„Î± Î±ÏÏ‡ÎµÎ¯Î±): 15-20 Î»ÎµÏ€Ï„Î¬
- **Chunks/second:** ~25-30

### ÎœÎµ CPU:
- **Lite version:** 30-45 Î»ÎµÏ€Ï„Î¬
- **Full version:** 60-90 Î»ÎµÏ€Ï„Î¬
- **Chunks/second:** ~5-8

---

## ğŸ†˜ Troubleshooting

### "CUDA Out of Memory" Ï€Î¬Î»Î¹

1. Check Ï„Î¹ Ï„ÏÎ­Ï‡ÎµÎ¹:
```bash
nvidia-smi
```

2. ÎšÎ»ÎµÎ¯ÏƒÎµ ÎŸÎ›Î‘ Ï„Î± GPU apps

3. Restart computer (clears GPU memory completely)

4. ÎœÎµÎ¯Ï‰ÏƒÎµ batch size ÏƒÎµ 32:
```python
# index_academicon_v2.py, line ~42
batch_size = 32
```

### "No module named 'config'"

```bash
cd D:\LOCAL-CODER
academicon-agent-env\Scripts\activate
```

### "Collection not found"

Î¤Î¿ database Î´ÎµÎ½ Î­Ï‡ÎµÎ¹ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î·Î¸ÎµÎ¯ Î±ÎºÏŒÎ¼Î±:
```bash
python index_academicon_lite.py
```

---

## ğŸ‰ Î•Ï€Î¹Ï„Ï…Ï‡Î¯Î±!

Î‘Î½ ÏŒÎ»Î± Î´Î¿ÏÎ»ÎµÏˆÎ±Î½:
1. âœ… Academicon codebase indexed
2. âœ… Database: `./academicon_chroma_db`
3. âœ… Ready Î³Î¹Î± queries!

**Î•Ï€ÏŒÎ¼ÎµÎ½Î¿ Î²Î®Î¼Î±:**
```bash
python web_ui.py
```

Î‘Î½Î¿Î¯Î³ÎµÎ¹ http://localhost:7860 ÎºÎ±Î¹ Î¼Ï€Î¿ÏÎµÎ¯Ï‚ Î½Î± ÎºÎ¬Î½ÎµÎ¹Ï‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚! ğŸš€

---

**Last Updated:** 2025-11-12
**Fixed by:** Claude Code
**Issue:** CUDA OOM Î¼Îµ batch_size=307
**Solution:** Conservative batch_size=64 Î³Î¹Î± large codebases
