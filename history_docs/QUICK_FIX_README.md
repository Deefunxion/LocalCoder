# ğŸ”§ CUDA OOM Fix - Quick Reference

## ğŸ“‹ Î¤Î¹ ÎˆÎ³Î¹Î½Îµ (12 ÎÎ¿Î­Î¼Î²ÏÎ· 2025)

**Î ÏÏŒÎ²Î»Î·Î¼Î±:** Academicon indexing Î­Ï€ÎµÏ†Ï„Îµ Î¼Îµ CUDA Out of Memory
- GPU batch size: 307 (Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î¿) âŒ
- VRAM exhaustion: Ollama LLM + Embeddings + 307 batch = OOM

**Î›ÏÏƒÎ·:** Conservative batch sizing
- GPU batch size: 64 (Î±ÏƒÏ†Î±Î»Î­Ï‚) âœ…
- Reserve 8GB Î³Î¹Î± Ollama
- Auto-stop Ollama Ï€ÏÎ¹Î½ indexing

---

## ğŸš€ Quick Start (3 Steps)

### 1ï¸âƒ£ Check VRAM
```bash
python check_vram.py
```

### 2ï¸âƒ£ Run Indexing
```bash
# Option A: Python only (faster)
index_academicon_lite.bat

# Option B: All files (complete)
index_academicon.bat

# Option C: Interactive menu
START_INDEXING.bat
```

### 3ï¸âƒ£ Test It
```bash
# Start Ollama (if stopped)
ollama serve

# Run assistant
python main.py
# OR
python web_ui.py
```

---

## ğŸ“Š What Changed

| File | Change | Impact |
|------|--------|--------|
| `gpu_utils.py` | Added `conservative` mode | Caps batch at 64 |
| `index_academicon_v2.py` | Uses conservative=True | Safe for large codebase |
| `index_academicon_lite.py` | batch_size 128â†’64 | Prevents OOM |

---

## ğŸ¯ Batch Size Guide

| Codebase Size | Old Batch | New Batch | Status |
|---------------|-----------|-----------|--------|
| Small (<5K) | 128 | 128 | âœ… OK |
| Medium (5-15K) | 128-307 | 64 | âœ… OK |
| **Academicon (25K)** | **307** | **64** | âœ… **FIXED!** |

---

## ğŸ“ New Files

- âœ… `check_vram.py` - VRAM checker
- âœ… `index_academicon.bat` - Auto full indexing  
- âœ… `index_academicon_lite.bat` - Auto lite indexing
- âœ… `START_INDEXING.bat` - Interactive menu
- âœ… `ACADEMICON_INDEXING_FIX.md` - Full docs (Greek)
- âœ… `GREEK_INDEXING_SUMMARY.md` - Summary (Greek)
- âœ… `QUICK_FIX_README.md` - This file

---

## âš¡ Performance

**GPU (RTX 5070 Ti) with batch_size=64:**
- Lite: 10-15 min âš¡
- Full: 15-20 min âš¡

**CPU (fallback) with batch_size=32:**
- Lite: 30-45 min ğŸŒ
- Full: 60-90 min ğŸŒ

---

## ğŸ†˜ Still Getting OOM?

### Solution 1: Lower Batch Size
Edit `index_academicon_v2.py` line ~42:
```python
batch_size = 32  # From 64 â†’ 32
```

### Solution 2: Use CPU
Edit `index_academicon_v2.py` line ~39:
```python
device = "cpu"
batch_size = 32
```

### Solution 3: Clear GPU Memory
```bash
# Restart computer (nuclear option)
# OR close all GPU apps
tasklist | findstr /i "ollama.exe chrome.exe"
```

---

## ğŸ“š Full Documentation

- **English:** `ACADEMICON_INDEXING_FIX.md` (detailed)
- **Î•Î»Î»Î·Î½Î¹ÎºÎ¬:** `GREEK_INDEXING_SUMMARY.md` (summary)
- **Implementation:** `IMPLEMENTATION_PROGRESS.md` (bug fixes section)

---

## âœ… Verification

```bash
# 1. Check no more batch_size=307 in code
findstr /s "batch_size = 307" *.py
# Should return: No matches

# 2. Run VRAM check
python check_vram.py
# Should show: batch_size recommendation = 64

# 3. Test indexing
index_academicon_lite.bat
# Should complete without OOM errors
```

---

**Status:** âœ… FIXED (2025-11-12)  
**Next Step:** Run `START_INDEXING.bat` and choose option 2 or 3! ğŸš€
