Î£Ï‡Î­Î´Î¹Î¿ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Local Multi-Agent Code Assistant

  ğŸ“‹ Î•Ï€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ· Î£Ï…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚

  ÎœÎµ Î²Î¬ÏƒÎ· Ï„Î· ÏƒÏ…Î¶Î®Ï„Î·ÏƒÎ· Î¼Îµ Ï„Î¿Î½ Manus, Ï„Î¿ "dream team" Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹:

  Agent 1 (Î‘ÏÏ‡ÎµÎ¹Î¿Î¸Î­Ï„Î·Ï‚): nomic-ai/nomic-embed-text-v1.5Agent 2 (Î‘Î½Î±Î»Ï…Ï„Î®Ï‚ Î£Ï‡Î­ÏƒÎµÏ‰Î½): Gemma-2-9B-it Î® Phi-3.5-mini-instructAgent 3 (Î£Ï…Î½Ï„Î¿Î½Î¹ÏƒÏ„Î®Ï‚):        
  Qwen2.5-14B-Instruct Î® DeepSeek R1Agent 4 (Î£Ï…Î½Î¸Î­Ï„Î·Ï‚): DeepSeek-Coder-V2-Lite-16B Î® Qwen2.5-Coder-7B

  ğŸ¯ Î£Ï„ÏŒÏ‡Î¿Ï‚

  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï„Î¿Ï€Î¹ÎºÎ¿Ï AI assistant Ï€Î¿Ï… Î½Î± "ÎºÎ±Ï„Î±Î»Î±Î²Î±Î¯Î½ÎµÎ¹" Ï€Î»Î®ÏÏ‰Ï‚ Ï„Î¿ Academicon codebase ÏƒÎ¿Ï… Î¼Î­ÏƒÏ‰ RAG (Retrieval-Augmented Generation) ÎºÎ±Î¹
  multi-agent ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚.

  ---
  Î¦Î‘Î£Î— 1: Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚

  Î’Î®Î¼Î± 1.1: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Hardware

  Î§ÏÏŒÎ½Î¿Ï‚: 15 Î»ÎµÏ€Ï„Î¬

  # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ GPU
  nvidia-smi

  # Î’ÎµÎ²Î±Î¯Ï‰ÏƒÎ·: 16GB VRAM (RTX 4090/5090) ÎºÎ±Î¹ 64GB RAM

  Î¤Î¹ Î½Î± ÎµÏ€Î¹Î²ÎµÎ²Î±Î¹ÏÏƒÎµÎ¹Ï‚:
  - GPU Î¼Îµ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ 16GB VRAM
  - 64GB RAM
  - 50-100GB ÎµÎ»ÎµÏÎ¸ÎµÏÎ¿Ï‚ Ï‡ÏÏÎ¿Ï‚ ÏƒÏ„Î¿ Î´Î¯ÏƒÎºÎ¿

  Î’Î®Î¼Î± 1.2: Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Ollama (Model Serving Framework)

  Î§ÏÏŒÎ½Î¿Ï‚: 10 Î»ÎµÏ€Ï„Î¬

  # Windows (PowerShell as Admin)
  winget install Ollama.Ollama

  # Î‰ ÎºÎ±Ï„Î­Î²Î±ÏƒÎµ Î±Ï€ÏŒ: https://ollama.com/download/windows

  # Î•Ï€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ· ÎµÎ³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·Ï‚
  ollama --version

  Î“Î¹Î±Ï„Î¯ Ollama: Î Î±Î½ÎµÏÎºÎ¿Î»Î¿ setup, OpenAI-compatible API, Î´Î¹Î±Ï‡ÎµÎ¹ÏÎ¯Î¶ÎµÏ„Î±Î¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Ï„Î± quantized models.

  Î’Î®Î¼Î± 1.3: Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Python Dependencies

  Î§ÏÏŒÎ½Î¿Ï‚: 10 Î»ÎµÏ€Ï„Î¬

  # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± virtual environment
  python -m venv academicon-agent-env
  .\academicon-agent-env\Scripts\activate  # Windows

  # Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î²Î±ÏƒÎ¹ÎºÏÎ½ Î²Î¹Î²Î»Î¹Î¿Î¸Î·ÎºÏÎ½
  pip install llama-index-core==0.10.x
  pip install llama-index-llms-ollama
  pip install llama-index-embeddings-huggingface
  pip install chromadb
  pip install sentence-transformers
  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

  ---
  Î¦Î‘Î£Î— 2: Î£Ï„Î®ÏƒÎ¹Î¼Î¿ Ï„Ï‰Î½ Agents (Download & Configuration)

  Î’Î®Î¼Î± 2.1: Agent 1 - Î‘ÏÏ‡ÎµÎ¹Î¿Î¸Î­Ï„Î·Ï‚ (Embedding Model)

  Î§ÏÏŒÎ½Î¿Ï‚: 20 Î»ÎµÏ€Ï„Î¬

  # Î˜Î± Î³Î¯Î½ÎµÎ¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± download ÏŒÏ„Î±Î½ Ï„Î¿ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚ Ï€ÏÏÏ„Î· Ï†Î¿ÏÎ¬
  from llama_index.embeddings.huggingface import HuggingFaceEmbedding

  embed_model = HuggingFaceEmbedding(
      model_name="nomic-ai/nomic-embed-text-v1.5",
      cache_folder="./models/embeddings"
  )

  # Test
  test_embedding = embed_model.get_text_embedding("def hello_world():")
  print(f"âœ“ Embedding dimension: {len(test_embedding)}")

  Î¤Î¹ ÎºÎ¬Î½ÎµÎ¹: ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ ÎºÏÎ´Î¹ÎºÎ± ÏƒÎµ Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ­Ï‚ Î±Î½Î±Ï€Î±ÏÎ±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ (vectors) Î³Î¹Î± Î³ÏÎ®Î³Î¿ÏÎ· Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·.

  Î’Î®Î¼Î± 2.2: Agent 2 - Î‘Î½Î±Î»Ï…Ï„Î®Ï‚ Î£Ï‡Î­ÏƒÎµÏ‰Î½ (Graph Analyst)

  Î§ÏÏŒÎ½Î¿Ï‚: 30 Î»ÎµÏ€Ï„Î¬

  # Download Phi-3.5-mini (Î³ÏÎ·Î³Î¿ÏÏŒÏ„ÎµÏÎ¿) Î® Gemma-2-9B (Î¹ÏƒÏ‡Ï…ÏÏŒÏ„ÎµÏÎ¿)
  ollama pull phi3.5:3.8b-mini-instruct-q4_K_M

  # Î‰
  ollama pull gemma2:9b-instruct-q4_K_M

  Test:
  from llama_index.llms.ollama import Ollama

  graph_analyst = Ollama(
      model="phi3.5:3.8b-mini-instruct-q4_K_M",
      base_url="http://localhost:11434",
      temperature=0.1  # Î§Î±Î¼Î·Î»Î® Î³Î¹Î± structured output
  )

  # Test structured output
  response = graph_analyst.complete(
      "Extract function names from this code as JSON: def login(): pass"
  )
  print(response)

  Î’Î®Î¼Î± 2.3: Agent 3 - Î£Ï…Î½Ï„Î¿Î½Î¹ÏƒÏ„Î®Ï‚ (Orchestrator)

  Î§ÏÏŒÎ½Î¿Ï‚: 45 Î»ÎµÏ€Ï„Î¬

  # Download Qwen2.5-14B (recommended Î³Î¹Î± 16GB VRAM)
  ollama pull qwen2.5:14b-instruct-q4_K_M

  # Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ¬ (Î±Î½ Ï„Î¿ 14B ÎµÎ¯Î½Î±Î¹ heavy):
  ollama pull deepseek-r1:8b-qwen-distilled-q4_K_M

  Test reasoning:
  orchestrator = Ollama(
      model="qwen2.5:14b-instruct-q4_K_M",
      base_url="http://localhost:11434",
      temperature=0.3
  )

  # Test planning ability
  response = orchestrator.complete("""
  You are a code analysis orchestrator. Break down this task:
  "Find all authentication-related functions in the codebase and explain their relationships."

  Provide a step-by-step plan.
  """)
  print(response)

  Î’Î®Î¼Î± 2.4: Agent 4 - Î£Ï…Î½Î¸Î­Ï„Î·Ï‚ (Final Answer Generator)

  Î§ÏÏŒÎ½Î¿Ï‚: 45 Î»ÎµÏ€Ï„Î¬

  # Download DeepSeek-Coder Î³Î¹Î± final synthesis
  ollama pull deepseek-coder:16b-base-q4_K_M

  # Î‰ Qwen2.5-Coder
  ollama pull qwen2.5-coder:7b-instruct-q4_K_M

  Test:
  synthesizer = Ollama(
      model="deepseek-coder:16b-base-q4_K_M",
      base_url="http://localhost:11434",
      temperature=0.7  # Î¥ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· Î³Î¹Î± creative explanations
  )

  response = synthesizer.complete("""
  Given this code context:
  [code snippet]

  Explain how it works in simple terms.
  """)
  print(response)

  ---
  Î¦Î‘Î£Î— 3: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Vector Database (Indexing Ï„Î¿Ï… Academicon)

  Î’Î®Î¼Î± 3.1: Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Codebase

  Î§ÏÏŒÎ½Î¿Ï‚: 15 Î»ÎµÏ€Ï„Î¬

  import os

  # ÎŒÏÎ¹ÏƒÎµ Ï„Î¿ path Ï„Î¿Ï… Academicon
  ACADEMICON_PATH = "C:/path/to/academicon"

  # Î•Ï€Î¹Î»Î¿Î³Î® Î±ÏÏ‡ÎµÎ¯Ï‰Î½ Ï€Î¿Ï… Î¸Î± indexÎ¬ÏÎµÎ¹Ï‚
  ALLOWED_EXTENSIONS = [".py", ".js", ".jsx", ".ts", ".tsx", ".vue", ".css", ".md"]

  def count_files(path, extensions):
      count = 0
      for root, dirs, files in os.walk(path):
          # Î‘Î³Î½ÏŒÎ·ÏƒÎµ node_modules, .git, virtual envs
          dirs[:] = [d for d in dirs if d not in ['node_modules', '.git', '__pycache__', 'venv']]
          for file in files:
              if any(file.endswith(ext) for ext in extensions):
                  count += 1
      return count

  total_files = count_files(ACADEMICON_PATH, ALLOWED_EXTENSIONS)
  print(f"âœ“ Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {total_files} Î±ÏÏ‡ÎµÎ¯Î± Ï€ÏÎ¿Ï‚ indexing")

  Î’Î®Î¼Î± 3.2: Chunking Strategy (Î£Ï€Î¬ÏƒÎ¹Î¼Î¿ ÎºÏÎ´Î¹ÎºÎ± ÏƒÎµ ÎºÎ¿Î¼Î¼Î¬Ï„Î¹Î±)

  Î§ÏÏŒÎ½Î¿Ï‚: 20 Î»ÎµÏ€Ï„Î¬

  from llama_index.core import SimpleDirectoryReader
  from llama_index.core.node_parser import CodeSplitter

  # Load documents
  documents = SimpleDirectoryReader(
      input_dir=ACADEMICON_PATH,
      recursive=True,
      required_exts=ALLOWED_EXTENSIONS,
      exclude_hidden=True,
      exclude=["node_modules", ".git", "dist", "build"]
  ).load_data()

  print(f"âœ“ Loaded {len(documents)} documents")

  # Code-aware chunking
  splitter = CodeSplitter(
      language="python",  # Î˜Î± Ï‡ÏÎµÎ¹Î±ÏƒÏ„ÎµÎ¯ Î­Î½Î± splitter Î±Î½Î¬ Î³Î»ÏÏƒÏƒÎ±
      chunk_lines=40,      # ~40 Î³ÏÎ±Î¼Î¼Î­Ï‚ Î±Î½Î¬ chunk
      chunk_lines_overlap=15,  # Overlap Î³Î¹Î± context
      max_chars=1500
  )

  nodes = splitter.get_nodes_from_documents(documents)
  print(f"âœ“ Created {len(nodes)} code chunks")

  Î’Î®Î¼Î± 3.3: Embedding & Storage ÏƒÏ„Î¿ ChromaDB

  Î§ÏÏŒÎ½Î¿Ï‚: 30-60 Î»ÎµÏ€Ï„Î¬ (Î±Î½Î¬Î»Î¿Î³Î± Î¼Îµ Ï„Î¿ Î¼Î­Î³ÎµÎ¸Î¿Ï‚ Ï„Î¿Ï… codebase)

  import chromadb
  from llama_index.core import VectorStoreIndex, StorageContext
  from llama_index.vector_stores.chroma import ChromaVectorStore
  from llama_index.core import Settings

  # Configure global settings
  Settings.embed_model = HuggingFaceEmbedding(
      model_name="nomic-ai/nomic-embed-text-v1.5"
  )

  # Setup ChromaDB
  chroma_client = chromadb.PersistentClient(path="./academicon_chroma_db")
  chroma_collection = chroma_client.get_or_create_collection("academicon_code")

  vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
  storage_context = StorageContext.from_defaults(vector_store=vector_store)

  # Build index (Î±Ï…Ï„ÏŒ Î¸Î± Ï€Î¬ÏÎµÎ¹ Ï‡ÏÏŒÎ½Î¿!)
  print("ğŸ”„ Building vector index... (Î±Ï…Ï„ÏŒ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€Î¬ÏÎµÎ¹ 30-60 Î»ÎµÏ€Ï„Î¬)")
  index = VectorStoreIndex(
      nodes,
      storage_context=storage_context,
      show_progress=True
  )

  print("âœ“ Index complete! Vector DB saved to ./academicon_chroma_db")

  Î ÏÎ¿ÏƒÎ¿Ï‡Î®: Î‘Ï…Ï„ÏŒ Î³Î¯Î½ÎµÏ„Î±Î¹ Î¼Î¯Î± Ï†Î¿ÏÎ¬. ÎœÎµÏ„Î¬ Ï„Î¿ index ÎµÎ¯Î½Î±Î¹ persistent.

  ---
  Î¦Î‘Î£Î— 4: Î¥Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Multi-Agent Orchestration

  Î’Î®Î¼Î± 4.1: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Agent Classes

  Î§ÏÏŒÎ½Î¿Ï‚: 45 Î»ÎµÏ€Ï„Î¬

  # agents.py

  from llama_index.llms.ollama import Ollama
  from llama_index.core import VectorStoreIndex
  from typing import List, Dict
  import json

  class IndexerAgent:
      """Agent 1: Retrieves relevant code chunks"""
      def __init__(self, index: VectorStoreIndex):
          self.index = index

      def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
          retriever = self.index.as_retriever(similarity_top_k=top_k)
          nodes = retriever.retrieve(query)

          return [{
              "text": node.node.text,
              "score": node.score,
              "metadata": node.node.metadata
          } for node in nodes]


  class GraphAnalystAgent:
      """Agent 2: Analyzes code relationships"""
      def __init__(self):
          self.llm = Ollama(
              model="phi3.5:3.8b-mini-instruct-q4_K_M",
              temperature=0.1
          )

      def analyze_relationships(self, code_chunks: List[Dict]) -> Dict:
          prompt = f"""
  Analyze these code chunks and extract:
  1. Function/class names
  2. Dependencies (imports, calls)
  3. File relationships

  Code chunks:
  {json.dumps([c['text'][:500] for c in code_chunks], indent=2)}

  Return a JSON with: {{"functions": [...], "dependencies": [...], "relationships": [...]}}
  """
          response = self.llm.complete(prompt)
          try:
              return json.loads(str(response))
          except:
              return {"functions": [], "dependencies": [], "relationships": []}


  class OrchestratorAgent:
      """Agent 3: Plans and coordinates"""
      def __init__(self):
          self.llm = Ollama(
              model="qwen2.5:14b-instruct-q4_K_M",
              temperature=0.3
          )

      def plan_query(self, user_query: str) -> Dict:
          prompt = f"""
  You are a code analysis orchestrator. Given this user query:
  "{user_query}"

  Create a search plan. Return JSON:
  {{
    "search_queries": ["query1", "query2"],
    "analysis_needed": true/false,
    "expected_file_types": [".py", ".js"]
  }}
  """
          response = self.llm.complete(prompt)
          try:
              return json.loads(str(response))
          except:
              return {
                  "search_queries": [user_query],
                  "analysis_needed": False,
                  "expected_file_types": []
              }


  class SynthesizerAgent:
      """Agent 4: Generates final answer"""
      def __init__(self):
          self.llm = Ollama(
              model="deepseek-coder:16b-base-q4_K_M",
              temperature=0.7
          )

      def synthesize(self, user_query: str, context: Dict) -> str:
          prompt = f"""
  You are an expert code assistant for the Academicon project.

  User Question: {user_query}

  Retrieved Code Context:
  {json.dumps(context.get('code_chunks', [])[:3], indent=2)}

  Code Analysis:
  {json.dumps(context.get('analysis', {}), indent=2)}

  Provide a clear, detailed answer based ONLY on this context.
  """
          response = self.llm.complete(prompt)
          return str(response)

  Î’Î®Î¼Î± 4.2: Î£ÏÎ½Î´ÎµÏƒÎ· Ï„Ï‰Î½ Agents (Main Pipeline)

  Î§ÏÏŒÎ½Î¿Ï‚: 30 Î»ÎµÏ€Ï„Î¬

  # main.py

  from agents import IndexerAgent, GraphAnalystAgent, OrchestratorAgent, SynthesizerAgent
  from llama_index.core import load_index_from_storage, StorageContext
  from llama_index.vector_stores.chroma import ChromaVectorStore
  import chromadb

  class AcademiconAssistant:
      def __init__(self, db_path="./academicon_chroma_db"):
          # Load existing index
          print("Loading vector index...")
          chroma_client = chromadb.PersistentClient(path=db_path)
          chroma_collection = chroma_client.get_collection("academicon_code")
          vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

          self.index = VectorStoreIndex.from_vector_store(vector_store)

          # Initialize agents
          self.indexer = IndexerAgent(self.index)
          self.graph_analyst = GraphAnalystAgent()
          self.orchestrator = OrchestratorAgent()
          self.synthesizer = SynthesizerAgent()

          print("âœ“ All agents ready!")

      def query(self, user_query: str) -> str:
          print(f"\nğŸ¤” User: {user_query}")

          # Step 1: Orchestrator plans
          print("ğŸ“‹ Orchestrator: Planning search strategy...")
          plan = self.orchestrator.plan_query(user_query)

          # Step 2: Indexer retrieves
          print(f"ğŸ” Indexer: Searching for relevant code...")
          all_chunks = []
          for search_query in plan.get('search_queries', [user_query]):
              chunks = self.indexer.retrieve(search_query, top_k=5)
              all_chunks.extend(chunks)

          # Step 3: Graph Analyst analyzes (if needed)
          analysis = {}
          if plan.get('analysis_needed', False) and all_chunks:
              print("ğŸ•µï¸ Graph Analyst: Analyzing relationships...")
              analysis = self.graph_analyst.analyze_relationships(all_chunks)

          # Step 4: Synthesizer creates answer
          print("âœï¸ Synthesizer: Generating answer...")
          context = {
              "code_chunks": all_chunks,
              "analysis": analysis
          }
          answer = self.synthesizer.synthesize(user_query, context)

          return answer


  # Usage
  if __name__ == "__main__":
      assistant = AcademiconAssistant()

      # Test query
      response = assistant.query("How does the CIP service work in Academicon?")
      print(f"\nğŸ¤– Assistant:\n{response}")

  ---
  Î¦Î‘Î£Î— 5: Testing & Optimization

  Î’Î®Î¼Î± 5.1: Î”Î¿ÎºÎ¹Î¼Î±ÏƒÏ„Î¹ÎºÎ­Ï‚ Î•ÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚

  Î§ÏÏŒÎ½Î¿Ï‚: 30 Î»ÎµÏ€Ï„Î¬

  # test_queries.py

  test_queries = [
      "What is the CIP service and how does it work?",
      "Show me the authentication flow in Academicon",
      "How are tasks managed in the task queue?",
      "Explain the database schema for user profiles",
      "What API endpoints are available for publications?"
  ]

  for query in test_queries:
      print(f"\n{'='*60}")
      response = assistant.query(query)
      print(f"Q: {query}")
      print(f"A: {response[:500]}...")  # First 500 chars

  Î’Î®Î¼Î± 5.2: Performance Monitoring

  Î§ÏÏŒÎ½Î¿Ï‚: 15 Î»ÎµÏ€Ï„Î¬

  import time

  def timed_query(assistant, query):
      start = time.time()
      response = assistant.query(query)
      elapsed = time.time() - start

      print(f"\nâ±ï¸ Query time: {elapsed:.2f}s")
      return response, elapsed

  # Test
  response, time_taken = timed_query(
      assistant,
      "Explain the CIP service"
  )

  Î‘Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½ÎµÏ‚ Î•Ï€Î¹Î´ÏŒÏƒÎµÎ¹Ï‚:
  - Retrieval (Indexer): 0.5-1s
  - Graph Analysis: 2-5s
  - Orchestration: 1-3s
  - Synthesis: 5-15s
  - Î£Ï…Î½Î¿Î»Î¹ÎºÏŒÏ‚ Ï‡ÏÏŒÎ½Î¿Ï‚ Î±Î½Î¬ query: 10-25s

  Î’Î®Î¼Î± 5.3: Optimizations (Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ)

  Î§ÏÏŒÎ½Î¿Ï‚: 60 Î»ÎµÏ€Ï„Î¬

  # optimizations.py

  # 1. Semantic Caching (Î³Î¹Î± ÏƒÏ…Ï‡Î½Î­Ï‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚)
  from functools import lru_cache
  import hashlib

  def cache_key(query: str) -> str:
      return hashlib.md5(query.encode()).hexdigest()

  @lru_cache(maxsize=100)
  def cached_retrieve(query_hash: str, index):
      # Implement caching logic
      pass

  # 2. Parallel Retrieval
  from concurrent.futures import ThreadPoolExecutor

  def parallel_retrieve(queries: List[str]) -> List[Dict]:
      with ThreadPoolExecutor(max_workers=3) as executor:
          results = list(executor.map(indexer.retrieve, queries))
      return [item for sublist in results for item in sublist]

  # 3. Batch Processing Î³Î¹Î± Ï€Î¿Î»Î»Î±Ï€Î»Î­Ï‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚
  def batch_query(assistant, queries: List[str]):
      return [assistant.query(q) for q in queries]

  ---
  Î¦Î‘Î£Î— 6: User Interface (Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ)

  Î’Î®Î¼Î± 6.1: Simple CLI Interface

  Î§ÏÏŒÎ½Î¿Ï‚: 20 Î»ÎµÏ€Ï„Î¬

  # cli.py

  def main():
      assistant = AcademiconAssistant()

      print("ğŸ¤– Academicon Code Assistant")
      print("Type 'exit' to quit\n")

      while True:
          query = input("You: ").strip()

          if query.lower() in ['exit', 'quit']:
              print("Goodbye!")
              break

          if not query:
              continue

          try:
              response = assistant.query(query)
              print(f"\nAssistant: {response}\n")
          except Exception as e:
              print(f"âŒ Error: {e}\n")

  if __name__ == "__main__":
      main()

  Î’Î®Î¼Î± 6.2: Web Interface Î¼Îµ Streamlit (Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ)

  Î§ÏÏŒÎ½Î¿Ï‚: 45 Î»ÎµÏ€Ï„Î¬

  pip install streamlit

  # app.py

  import streamlit as st
  from main import AcademiconAssistant

  @st.cache_resource
  def load_assistant():
      return AcademiconAssistant()

  st.title("ğŸ¤– Academicon Code Assistant")

  if 'assistant' not in st.session_state:
      with st.spinner("Loading models..."):
          st.session_state.assistant = load_assistant()

  query = st.text_input("Ask about your codebase:")

  if st.button("Ask") and query:
      with st.spinner("Thinking..."):
          response = st.session_state.assistant.query(query)
          st.markdown(f"**Answer:**\n\n{response}")

  # Run with: streamlit run app.py

  ---
  Î¦Î‘Î£Î— 7: Î£Ï…Î½Ï„Î®ÏÎ·ÏƒÎ· & Updates

  Î’Î®Î¼Î± 7.1: Re-indexing Strategy

  Î§ÏÏŒÎ½Î¿Ï‚: Ongoing

  # update_index.py

  def incremental_update(new_files_path: str):
      """Update index with new files only"""
      from llama_index.core import SimpleDirectoryReader

      new_docs = SimpleDirectoryReader(
          input_dir=new_files_path,
          recursive=True
      ).load_data()

      # Add to existing index
      for doc in new_docs:
          index.insert(doc)

      print(f"âœ“ Added {len(new_docs)} new documents")

  # Run ÎµÎ²Î´Î¿Î¼Î±Î´Î¹Î±Î¯Î± Î® ÏŒÏ„Î±Î½ ÎºÎ¬Î½ÎµÎ¹Ï‚ Î¼ÎµÎ³Î¬Î»ÎµÏ‚ Î±Î»Î»Î±Î³Î­Ï‚ ÏƒÏ„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ±

  Î’Î®Î¼Î± 7.2: Model Updates

  Î§ÏÏŒÎ½Î¿Ï‚: Ongoing

  # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± Î½Î­ÎµÏ‚ ÎµÎºÎ´ÏŒÏƒÎµÎ¹Ï‚ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½
  ollama list

  # Update models
  ollama pull qwen2.5:14b-instruct-q4_K_M
  ollama pull deepseek-coder:16b-base-q4_K_M

  ---
  ğŸ“Š Î£Ï…Î½Î¿Î»Î¹ÎºÏŒÏ‚ Î§ÏÏŒÎ½Î¿Ï‚ Î¥Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚

  | Î¦Î¬ÏƒÎ·                     | Î§ÏÏŒÎ½Î¿Ï‚   | Î ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î±  |
  |--------------------------|----------|----------------|
  | Î¦Î¬ÏƒÎ· 1: Setup            | 35 Î»ÎµÏ€Ï„Î¬ | ğŸ”´ ÎšÏÎ¯ÏƒÎ¹Î¼Î·     |
  | Î¦Î¬ÏƒÎ· 2: Agent Downloads  | 2-3 ÏÏÎµÏ‚ | ğŸ”´ ÎšÏÎ¯ÏƒÎ¹Î¼Î·     |
  | Î¦Î¬ÏƒÎ· 3: Indexing         | 1-2 ÏÏÎµÏ‚ | ğŸ”´ ÎšÏÎ¯ÏƒÎ¹Î¼Î·     |
  | Î¦Î¬ÏƒÎ· 4: Multi-Agent Code | 1.5 ÏÏÎµÏ‚ | ğŸ”´ ÎšÏÎ¯ÏƒÎ¹Î¼Î·     |
  | Î¦Î¬ÏƒÎ· 5: Testing          | 1.5 ÏÏÎµÏ‚ | ğŸŸ¡ Î£Î·Î¼Î±Î½Ï„Î¹ÎºÎ®   |
  | Î¦Î¬ÏƒÎ· 6: UI               | 1-2 ÏÏÎµÏ‚ | ğŸŸ¢ Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ® |
  | Î¦Î¬ÏƒÎ· 7: Maintenance      | Ongoing  | ğŸŸ¡ Î£Î·Î¼Î±Î½Ï„Î¹ÎºÎ®   |

  Î£Ï…Î½Î¿Î»Î¹ÎºÏŒ: 7-12 ÏÏÎµÏ‚ (1-2 Î£Î±Î²Î²Î±Ï„Î¿ÎºÏÏÎ¹Î±ÎºÎ±)

  ---
  ğŸš€ Next Steps (ÎœÎµÏ„Î¬ Ï„Î¿ Î’Î±ÏƒÎ¹ÎºÏŒ Setup)

  1. Advanced Retrieval: Hybrid search (BM25 + Vector)
  2. Agent Memory: Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· conversation history
  3. Code Execution: Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Python REPL Î³Î¹Î± testing ÎºÏÎ´Î¹ÎºÎ±
  4. Documentation Generation: Auto-generate docs Î±Ï€ÏŒ Ï„Î¿ codebase
  5. Fine-tuning: LoRA fine-tuning Î³Î¹Î± Academicon-specific patterns

  ---